<org.apache.myfaces.shared.util.StateUtils: java.lang.Object reconstruct(java.lang.String,javax.faces.context.ExternalContext)>
<org.apache.myfaces.shared.util.StateUtils: byte[] encrypt(byte[],javax.faces.context.ExternalContext)>
<org.apache.myfaces.shared.util.StateUtils: java.lang.Object reconstruct(java.lang.String,javax.faces.context.ExternalContext)>
<org.apache.myfaces.shared.util.StateUtils: java.lang.Object reconstruct(java.lang.String,javax.faces.context.ExternalContext)>
<org.apache.activemq.artemis.utils.DefaultSensitiveStringCodec$BlowfishAlgorithm: java.lang.String encode(java.lang.String)>
<org.apache.activemq.artemis.utils.DefaultSensitiveStringCodec: java.lang.Object encode(java.lang.Object)>
<org.apache.activemq.artemis.utils.SecureHashProcessor: java.lang.String hash(java.lang.String)>
<org.apache.spark.util.Utils$: java.util.Random randomizeInPlace$default$2()>
<org.apache.spark.util.Utils: java.util.Random randomizeInPlace$default$2()>
<org.apache.spark.util.Utils$: scala.collection.Seq randomize(scala.collection.TraversableOnce,scala.reflect.ClassTag)>
<org.apache.spark.util.Utils: scala.collection.Seq randomize(scala.collection.TraversableOnce,scala.reflect.ClassTag)>
<org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.RDD$$anonfun$7: scala.collection.Iterator apply()>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1: scala.collection.GenTraversableOnce apply(scala.Tuple2)>
<org.apache.spark.rdd.MapPartitionsRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1: scala.collection.Iterator apply(java.lang.Object)>
<org.apache.spark.rdd.ZippedPartitionsRDD4: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.UnionRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.api.python.PythonRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1: scala.collection.Iterator apply(org.apache.spark.Partition)>
<org.apache.spark.rdd.ZippedWithIndexRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.CartesianRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.scheduler.ResultTask: java.lang.Object runTask(org.apache.spark.TaskContext)>
<org.apache.spark.api.r.BaseRRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.ZippedPartitionsRDD3: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.api.python.PairwiseRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.ZippedPartitionsRDD2: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.PartitionPruningRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.NewHadoopRDD$NewHadoopMapPartitionsWithSplitRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.PartitionwiseSampledRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.api.java.JavaRDDLike$class: java.util.Iterator iterator(org.apache.spark.api.java.JavaRDDLike,org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.HadoopRDD$HadoopMapPartitionsWithSplitRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.RDD$$anonfun$7: java.lang.Object apply()>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.ShuffleMapTask: java.lang.Object runTask(org.apache.spark.TaskContext)>
<org.apache.spark.api.java.AbstractJavaRDDLike: java.util.Iterator iterator(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.spark_project.jetty.server.session.JDBCSessionManager: org.spark_project.jetty.server.session.JDBCSessionManager$Session getSession(java.lang.String)>
<org.spark_project.jetty.server.session.JDBCSessionManager: org.spark_project.jetty.server.session.AbstractSession getSession(java.lang.String)>
<org.spark_project.jetty.io.ByteArrayEndPoint: java.lang.String takeOutputString(java.nio.charset.Charset)>
<org.spark_project.jetty.http.ResourceHttpContent: java.nio.ByteBuffer getDirectBuffer()>
<org.spark_project.jetty.http.ResourceHttpContent: java.nio.ByteBuffer getIndirectBuffer()>
<org.spark_project.jetty.io.WriteFlusher: boolean onFail(java.lang.Throwable)>
<org.spark_project.jetty.client.util.DeferredContentProvider: boolean offer(java.nio.ByteBuffer,org.spark_project.jetty.util.Callback)>
<org.spark_project.jetty.server.LocalConnector: org.spark_project.jetty.server.LocalConnector$LocalEndPoint executeRequest(java.lang.String)>
<org.spark_project.jetty.io.ByteArrayEndPoint: java.lang.String takeOutputString()>
<org.spark_project.jetty.server.ResourceCache$CachedHttpContent: java.nio.ByteBuffer getIndirectBuffer()>
<org.spark_project.jetty.server.ResourceCache$CachedHttpContent: java.nio.ByteBuffer getDirectBuffer()>
<org.spark_project.jetty.io.AbstractEndPoint: boolean tryFillInterested(org.spark_project.jetty.util.Callback)>
<org.spark_project.jetty.client.util.DeferredContentProvider: boolean offer(java.nio.ByteBuffer)>
<org.spark_project.jetty.server.LocalConnector: java.lang.String getResponses(java.lang.String,long,java.util.concurrent.TimeUnit)>
<org.spark_project.jetty.server.LocalConnector: java.nio.ByteBuffer getResponses(java.nio.ByteBuffer)>
<org.spark_project.jetty.server.LocalConnector: java.nio.ByteBuffer getResponse(java.nio.ByteBuffer)>
<org.spark_project.jetty.server.LocalConnector: java.lang.String getResponse(java.lang.String)>
<org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anonfun$apply$mcV$sp$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$2$$anonfun$run$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$2$$anonfun$run$4: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$cancelLastRegistrationRetry$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anonfun$apply$mcV$sp$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$onStop$1: java.lang.Object apply(java.lang.Object)>
<org.spark_project.jetty.client.HttpClient: org.spark_project.jetty.client.api.ContentResponse GET(java.lang.String)>
<org.spark_project.jetty.client.HttpClient: org.spark_project.jetty.client.api.ContentResponse FORM(java.lang.String,org.spark_project.jetty.util.Fields)>
<org.apache.spark.util.Utils: java.lang.Object tryWithResource(scala.Function0,scala.Function1)>
<org.apache.spark.storage.DiskBlockData: org.apache.spark.util.io.ChunkedByteBuffer toChunkedByteBuffer(scala.Function1)>
<org.apache.spark.storage.DiskBlockData: java.nio.ByteBuffer toByteBuffer()>
<org.spark_project.jetty.server.LocalConnector: java.lang.String getResponses(java.lang.String)>
<org.apache.spark.deploy.history.FsHistoryProvider: scala.Option getAppUI(java.lang.String,scala.Option)>
<org.apache.spark.status.api.v1.StagesResource$$anonfun$taskSummary$1: org.apache.spark.status.api.v1.TaskMetricDistributions apply(org.apache.spark.ui.SparkUI)>
<org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$close$2: org.apache.spark.storage.FileSegment apply()>
<org.spark_project.jetty.server.Request: java.lang.String getParameter(java.lang.String)>
<org.spark_project.jetty.server.Request: java.util.Map getParameterMap()>
<org.spark_project.jetty.server.Request: java.util.Enumeration getParameterNames()>
<org.spark_project.jetty.server.Request: java.lang.String[] getParameterValues(java.lang.String)>
<org.spark_project.jetty.client.DuplexConnectionPool: org.spark_project.jetty.client.api.Connection acquire()>
<org.spark_project.jetty.client.DuplexConnectionPool: boolean release(org.spark_project.jetty.client.api.Connection)>
<org.apache.spark.rpc.netty.NettyRpcEndpointRef: scala.concurrent.Future ask(java.lang.Object,org.apache.spark.rpc.RpcTimeout,scala.reflect.ClassTag)>
<org.apache.spark.status.api.v1.StagesResource$$anonfun$taskSummary$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager$$anonfun$26: java.lang.Object apply()>
<org.apache.spark.rpc.netty.NettyRpcEnv: java.nio.ByteBuffer serialize(java.lang.Object)>
<org.apache.spark.serializer.SerializerManager: org.apache.spark.util.io.ChunkedByteBuffer dataSerialize(org.apache.spark.storage.BlockId,scala.collection.Iterator,scala.reflect.ClassTag)>
<org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$close$2: java.lang.Object apply()>
<org.spark_project.jetty.client.PoolingHttpDestination: org.spark_project.jetty.client.api.Connection acquire()>
<org.apache.spark.rpc.RpcEnv$: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,int,boolean)>
<org.spark_project.jetty.client.HttpChannel: boolean abortResponse(org.spark_project.jetty.client.HttpExchange,java.lang.Throwable)>
<org.apache.spark.util.Utils$$anonfun$copyStream$1: long apply()>
<org.apache.spark.ui.WebUI$$anonfun$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$foldByKey$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1: java.lang.Object apply()>
<org.apache.spark.scheduler.TaskSetManager$$anonfun$resourceOffer$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.RDD$$anonfun$fold$1: java.lang.Object apply()>
<org.apache.spark.util.Utils: java.lang.Object clone(java.lang.Object,org.apache.spark.serializer.SerializerInstance,scala.reflect.ClassTag)>
<org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$aggregate$1: java.lang.Object apply()>
<org.apache.spark.scheduler.Task: byte[] $lessinit$greater$default$5()>
<org.apache.spark.storage.BlockManager$$anonfun$getLocalBytes$2: org.apache.spark.storage.BlockData apply(org.apache.spark.storage.BlockInfo)>
<org.apache.spark.SparkContext: org.apache.spark.SparkContext getOrCreate(org.apache.spark.SparkConf)>
<org.spark_project.jetty.server.Request: java.lang.String getRemoteUser()>
<org.apache.spark.unsafe.map.BytesToBytesMap: long spill(long,org.apache.spark.memory.MemoryConsumer)>
<org.apache.spark.ui.jobs.AllJobsPage: scala.collection.Seq render(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.deploy.worker.ui.WorkerWebUI$$anonfun$initialize$1: java.lang.String apply(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$onStart$2: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$5: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.HeartbeatReceiver$$anonfun$removeExecutor$1: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.HeartbeatReceiver$$anonfun$addExecutor$1: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.scheduler.OutputCommitCoordinator: boolean canCommit(int,int,int)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockFromWorkers$1: java.lang.Object apply(org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeShuffle$1: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.deploy.client.StandaloneAppClient: scala.concurrent.Future requestTotalExecutors(int)>
<org.apache.spark.deploy.client.StandaloneAppClient: scala.concurrent.Future killExecutors(scala.collection.Seq)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockManager$2: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBroadcast$1: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$getMatchingBlockIds$1: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.HeartbeatReceiver$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.scheduler.DAGScheduler: boolean executorHeartbeatReceived(java.lang.String,scala.Tuple4[],org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.rpc.RpcEndpointRef: java.lang.Object askSync(java.lang.Object,scala.reflect.ClassTag)>
<org.apache.spark.rpc.RpcEnv: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,int,boolean)>
<org.apache.spark.rpc.RpcEnv$: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)>
<org.apache.spark.scheduler.TaskDescription: java.nio.ByteBuffer encode(org.apache.spark.scheduler.TaskDescription)>
<org.apache.spark.util.Utils$$anonfun$copyStream$1: java.lang.Object apply()>
<org.apache.spark.deploy.history.ApplicationCache$$anonfun$1: java.lang.Object apply()>
<org.apache.spark.storage.memory.MemoryStore: scala.util.Either putIteratorAsValues(org.apache.spark.storage.BlockId,scala.collection.Iterator,scala.reflect.ClassTag)>
<org.apache.spark.storage.memory.MemoryStore: scala.util.Either putIteratorAsBytes(org.apache.spark.storage.BlockId,scala.collection.Iterator,scala.reflect.ClassTag,org.apache.spark.memory.MemoryMode)>
<org.apache.spark.storage.BlockManager$$anonfun$getLocalBytes$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: org.apache.spark.SparkContext getOrCreate()>
<org.apache.spark.api.r.RRDD: org.apache.spark.api.java.JavaSparkContext createSparkContext(java.lang.String,java.lang.String,java.lang.String,java.lang.String[],java.util.Map,java.util.Map)>
<org.apache.spark.util.collection.ExternalSorter: boolean forceSpill()>
<org.apache.spark.memory.MemoryConsumer: long acquireMemory(long)>
<org.apache.spark.deploy.worker.ui.WorkerWebUI$$anonfun$initialize$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$onStart$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$blockStatus$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anonfun$removeExecutor$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anonfun$addExecutor$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockFromWorkers$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeShuffle$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend: scala.concurrent.Future doRequestTotalExecutors(int)>
<org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend: scala.concurrent.Future doKillExecutors(scala.collection.Seq)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockManager$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBroadcast$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$getMatchingBlockIds$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.TaskSchedulerImpl: boolean executorHeartbeatReceived(java.lang.String,scala.Tuple2[],org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.storage.BlockManagerMaster: org.apache.spark.storage.BlockManagerId registerBlockManager(org.apache.spark.storage.BlockManagerId,long,long,org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.storage.BlockManagerMaster: boolean updateBlockInfo(org.apache.spark.storage.BlockManagerId,org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel,long,long)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.Seq getLocations(org.apache.spark.storage.BlockId)>
<org.apache.spark.storage.BlockManagerMaster: scala.Option getLocationsAndStatus(org.apache.spark.storage.BlockId)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.IndexedSeq getLocations(org.apache.spark.storage.BlockId[])>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.Seq getPeers(org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.storage.BlockManagerMaster: scala.Option getExecutorEndpointRef(java.lang.String)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.immutable.Map getMemoryStatus()>
<org.apache.spark.storage.BlockManagerMaster: org.apache.spark.storage.StorageStatus[] getStorageStatus()>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.immutable.Map getBlockStatus(org.apache.spark.storage.BlockId,boolean)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.Seq getMatchingBlockIds(scala.Function1,boolean)>
<org.apache.spark.storage.BlockManagerMaster: boolean hasCachedBlocks(java.lang.String)>
<org.apache.spark.deploy.master.ui.MasterWebUI: scala.Option idToUiAddress(java.lang.String)>
<org.apache.spark.MapOutputTracker: java.lang.Object askTracker(java.lang.Object,scala.reflect.ClassTag)>
<org.apache.spark.deploy.master.ui.MasterPage: org.apache.spark.deploy.DeployMessages$MasterStateResponse getMasterState()>
<org.apache.spark.deploy.worker.ui.WorkerPage: org.json4s.JsonAST$JValue renderJson(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.deploy.worker.ui.WorkerPage: scala.collection.Seq render(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.SparkEnv$: org.apache.spark.SparkEnv createDriverEnv(org.apache.spark.SparkConf,boolean,org.apache.spark.scheduler.LiveListenerBus,int,scala.Option)>
<org.apache.spark.SparkEnv$: org.apache.spark.SparkEnv createExecutorEnv(org.apache.spark.SparkConf,java.lang.String,java.lang.String,int,scala.Option,boolean)>
<org.apache.spark.deploy.worker.Worker$: org.apache.spark.rpc.RpcEnv startRpcEnvAndEndpoint(java.lang.String,int,int,int,int,java.lang.String[],java.lang.String,scala.Option,org.apache.spark.SparkConf)>
<org.apache.spark.rpc.RpcEnv: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)>
<org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager: scala.util.Either getOrElseUpdate(org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel,scala.reflect.ClassTag,scala.Function0)>
<org.apache.spark.storage.BlockManager: scala.Option getSingle(org.apache.spark.storage.BlockId,scala.reflect.ClassTag)>
<org.apache.spark.rdd.BlockRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.executor.TaskMetrics$$anonfun$nonZeroInternalAccums$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager$$anonfun$doPutBytes$1: scala.Option apply(org.apache.spark.storage.BlockInfo)>
<org.apache.spark.status.api.v1.BaseAppResource$$anonfun$withUI$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: boolean killTaskAttempt(long,boolean,java.lang.String)>
<org.apache.spark.storage.BlockManager: boolean org$apache$spark$storage$BlockManager$$tryToReportBlockStatus(org.apache.spark.storage.BlockId,org.apache.spark.storage.BlockStatus,long)>
<org.apache.spark.scheduler.DAGScheduler: scala.collection.IndexedSeq getCacheLocs(org.apache.spark.rdd.RDD)>
<org.apache.spark.storage.BlockManager: scala.collection.Seq[] org$apache$spark$storage$BlockManager$$getLocationBlockIds(org.apache.spark.storage.BlockId[])>
<org.apache.spark.SparkContext: scala.collection.Map getExecutorMemoryStatus()>
<org.apache.spark.SparkContext: org.apache.spark.storage.StorageStatus[] getExecutorStorageStatus()>
<org.apache.spark.storage.BlockManagerSource$$anonfun$5: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$2: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$6: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$4: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$10: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$3: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$1: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$7: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$8: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$9: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.deploy.master.ui.MasterWebUI$$anonfun$3: scala.Option apply(java.lang.String)>
<org.apache.spark.deploy.LocalSparkCluster: java.lang.String[] start()>
<org.apache.spark.deploy.master.Master: scala.Tuple3 startRpcEnvAndEndpoint(java.lang.String,int,int,org.apache.spark.SparkConf)>
<org.apache.spark.deploy.master.ui.MasterPage: org.json4s.JsonAST$JValue renderJson(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.deploy.master.ui.MasterPage: scala.collection.Seq render(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.SparkContext: org.apache.spark.SparkEnv createSparkEnv(org.apache.spark.SparkConf,boolean,org.apache.spark.scheduler.LiveListenerBus)>
<org.apache.spark.deploy.worker.Worker: org.apache.spark.rpc.RpcEnv startRpcEnvAndEndpoint(java.lang.String,int,int,int,int,java.lang.String[],java.lang.String,scala.Option,org.apache.spark.SparkConf)>
<org.apache.spark.deploy.LocalSparkCluster$$anonfun$start$2: scala.collection.mutable.ArrayBuffer apply(int)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$client$StandaloneAppClient$ClientEndpoint$$askAndReplyAsync$1: java.lang.Object applyOrElse(java.lang.Object,scala.Function1)>
<org.apache.spark.SparkContext: org.apache.spark.partial.PartialResult runApproximateJob(org.apache.spark.rdd.RDD,scala.Function2,org.apache.spark.partial.ApproximateEvaluator,long)>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$map$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$reduce$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1: scala.collection.mutable.Map apply()>
<org.apache.spark.rdd.RDD$$anonfun$filter$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$keyBy$1: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.SparkContext$$anonfun$sequenceFile$3: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$treeReduce$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreach$1: scala.runtime.BoxedUnit[] apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$flatMapValues$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$collect$2: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1: scala.runtime.BoxedUnit[] apply()>
<org.apache.spark.rdd.RDD$$anonfun$groupBy$3: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$flatMap$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.storage.BlockManager$$anonfun$doPutBytes$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$3: scala.collection.Seq apply(scala.collection.Set)>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$$anonfun$13: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.DAGScheduler: scala.collection.Seq org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal(org.apache.spark.rdd.RDD,int,scala.collection.mutable.HashSet)>
<org.apache.spark.storage.BlockManager: scala.collection.immutable.Map blockIdsToHosts(org.apache.spark.storage.BlockId[],org.apache.spark.SparkEnv,org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$6: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$4: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$10: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$7: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$8: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$9: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.master.ui.MasterWebUI$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.rest.StatusRequestServlet$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.rest.KillRequestServlet$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.ui.WebUI$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.LocalSparkCluster$$anonfun$start$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulable accumulable(java.lang.Object,java.lang.String,org.apache.spark.AccumulableParam)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator intAccumulator(int)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator doubleAccumulator(double)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(java.lang.Object,org.apache.spark.AccumulatorParam)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator intAccumulator(int,java.lang.String)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator doubleAccumulator(double,java.lang.String)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(java.lang.Object,java.lang.String,org.apache.spark.AccumulatorParam)>
<org.apache.spark.SparkContext: java.lang.Object runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.reflect.ClassTag)>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$lookup$1: scala.collection.Seq apply()>
<org.apache.spark.rdd.RDD$$anonfun$toLocalIterator$1: java.lang.Object org$apache$spark$rdd$RDD$$anonfun$$collectPartition$1(int)>
<org.apache.spark.rdd.RDD$$anonfun$take$1: java.lang.Object apply()>
<org.apache.spark.api.java.JavaRDDLike$class: java.util.List[] collectPartitions(org.apache.spark.api.java.JavaRDDLike,int[])>
<org.apache.spark.api.python.PythonRDD$: int runJob(org.apache.spark.SparkContext,org.apache.spark.api.java.JavaRDD,java.util.ArrayList)>
<org.apache.spark.rdd.RDD$$anonfun$countByValueApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sumApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.rdd.RDD$$anonfun$countApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$meanApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.ComplexFutureAction$$anon$1: org.apache.spark.FutureAction submitJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.Function2,scala.Function0)>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachPartitionAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$collectAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$countAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$map$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$zipPartitions$5: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$filter$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$keyBy$1: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$sequenceFile$3: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreach$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$pipe$3: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$zipPartitions$3: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$flatMapValues$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$collect$2: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$zipPartitions$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKeyWithClassTag$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$groupBy$3: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$flatMap$1: java.lang.Object apply()>
<org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2: java.lang.Object apply()>
<org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4: java.lang.Object apply()>
<org.apache.spark.SparkContext: boolean killExecutor(java.lang.String)>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend: boolean killExecutor(java.lang.String)>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.LocalRDDCheckpointData$$anonfun$1: boolean apply(int)>
<org.apache.spark.scheduler.DAGScheduler: scala.collection.Seq getPreferredLocs(org.apache.spark.rdd.RDD,int)>
<org.apache.spark.api.java.JavaPairRDD: org.apache.spark.api.java.JavaPairRDD unpersist()>
<org.apache.spark.api.java.JavaPairRDD: org.apache.spark.api.java.JavaPairRDD unpersist(boolean)>
<org.apache.spark.api.java.JavaDoubleRDD: org.apache.spark.api.java.JavaDoubleRDD unpersist()>
<org.apache.spark.api.java.JavaDoubleRDD: org.apache.spark.api.java.JavaDoubleRDD unpersist(boolean)>
<org.apache.spark.api.java.JavaRDD: org.apache.spark.api.java.JavaRDD unpersist()>
<org.apache.spark.api.java.JavaRDD: org.apache.spark.api.java.JavaRDD unpersist(boolean)>
<org.apache.spark.SparkContext: org.apache.spark.storage.RDDInfo[] getRDDStorageInfo()>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulable accumulable(java.lang.Object,org.apache.spark.AccumulableParam)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(int)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(double)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(int,java.lang.String)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(double,java.lang.String)>
<org.apache.spark.SparkContext: java.lang.Object runJob(org.apache.spark.rdd.RDD,scala.Function2,scala.reflect.ClassTag)>
<org.apache.spark.rdd.RDD: long count()>
<org.apache.spark.rdd.RDD$$anonfun$collectPartitions$1: java.lang.Object[] apply()>
<org.apache.spark.rdd.RDD$$anonfun$collect$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$lookup$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$toLocalIterator$1$$anonfun$apply$30: scala.collection.mutable.ArrayOps apply(int)>
<org.apache.spark.api.java.AbstractJavaRDDLike: java.util.List[] collectPartitions(int[])>
<org.apache.spark.api.python.PythonRDD: int runJob(org.apache.spark.SparkContext,org.apache.spark.api.java.JavaRDD,java.util.ArrayList)>
<org.apache.spark.rdd.RDD$$anonfun$countByValueApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sumApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$countApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$meanApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachPartitionAsync$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$collectAsync$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$countAsync$1: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$hadoopFile$1: java.lang.Object apply()>
<org.apache.spark.rdd.LocalRDDCheckpointData$$anonfun$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: scala.collection.Seq getPreferredLocs(org.apache.spark.rdd.RDD,int)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobSubmitted$5: java.lang.String apply()>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleMapStageSubmitted$5: java.lang.String apply()>
<org.apache.spark.rdd.BlockRDD: scala.collection.Seq getPreferredLocations(org.apache.spark.Partition)>
<org.apache.spark.rdd.BlockRDD: scala.collection.immutable.Map getBlockIdLocations()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$7: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$1: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$5: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$6: long apply$mcJ$sp()>
<org.apache.spark.rdd.RDD$$anonfun$takeSample$1: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$3: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$4: long apply$mcJ$sp()>
<org.apache.spark.api.java.JavaRDDLike$class: long count(org.apache.spark.api.java.JavaRDDLike)>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$2: long apply$mcJ$sp()>
<org.apache.spark.rdd.RDD$$anonfun$collectPartitions$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$toLocalIterator$1$$anonfun$apply$30: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.RDD$$anonfun$zipWithIndex$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$org$apache$spark$rdd$AsyncRDDActions$$anonfun$$continue$1$1: scala.concurrent.Future apply(scala.Unit$)>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$11: scala.concurrent.Future apply(org.apache.spark.JobSubmitter)>
<org.apache.spark.SparkContext$$anonfun$hadoopRDD$1: java.lang.Object apply()>
<org.apache.spark.rdd.DefaultPartitionCoalescer: scala.collection.Seq currPrefLocs(org.apache.spark.Partition,org.apache.spark.rdd.RDD)>
<org.apache.spark.rdd.PartitionerAwareUnionRDD: scala.collection.Seq org$apache$spark$rdd$PartitionerAwareUnionRDD$$currPrefLocs(org.apache.spark.rdd.RDD,org.apache.spark.Partition)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$15: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$16: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobSubmitted$5: java.lang.Object apply()>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleMapStageSubmitted$5: java.lang.Object apply()>
<org.apache.spark.rdd.RDD: java.lang.String toDebugString()>
<org.apache.spark.rdd.RDD$$anonfun$38: scala.collection.Seq apply(org.apache.spark.Dependency)>
<org.apache.spark.rdd.ReliableRDDCheckpointData: org.apache.spark.rdd.CheckpointRDD doCheckpoint()>
<org.apache.spark.rdd.ReliableCheckpointRDD: org.apache.spark.rdd.ReliableCheckpointRDD writeRDDToCheckpointDirectory(org.apache.spark.rdd.RDD,java.lang.String,int,scala.reflect.ClassTag)>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$7: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$1: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$5: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$6: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$3: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$4: long apply()>
<org.apache.spark.api.java.AbstractJavaRDDLike: long count()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$2: long apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$org$apache$spark$rdd$AsyncRDDActions$$anonfun$$continue$1$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$11: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager: boolean putBytes(org.apache.spark.storage.BlockId,org.apache.spark.util.io.ChunkedByteBuffer,org.apache.spark.storage.StorageLevel,boolean,scala.reflect.ClassTag)>
<org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1: boolean apply()>
<org.apache.spark.rdd.CoalescedRDDPartition$$anonfun$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations$$anonfun$getAllPrefLocs$2: java.lang.Object apply(org.apache.spark.Partition)>
<org.apache.spark.rdd.DefaultPartitionCoalescer: org.apache.spark.rdd.PartitionGroup pickBin(org.apache.spark.Partition,org.apache.spark.rdd.RDD,double,org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations)>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$5: scala.collection.Seq apply(scala.Tuple2)>
<org.apache.spark.rdd.RDD$$anonfun$preferredLocations$2: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$runJob$2: java.lang.String apply()>
<org.apache.spark.api.java.JavaRDDLike$class: java.lang.String toDebugString(org.apache.spark.api.java.JavaRDDLike)>
<org.apache.spark.rdd.RDD$$anonfun$38: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$7: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$1: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$5: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$6: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$3: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$4: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$2: java.lang.Object apply()>
<org.apache.spark.storage.BlockManager: boolean putSingle(org.apache.spark.storage.BlockId,java.lang.Object,org.apache.spark.storage.StorageLevel,boolean,scala.reflect.ClassTag)>
<org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1: java.lang.Object apply()>
<org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations$$anonfun$getAllPrefLocs$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.DefaultPartitionCoalescer$$anonfun$throwBalls$9: scala.collection.mutable.ArrayBuffer apply(org.apache.spark.Partition)>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext$$anonfun$runJob$2: java.lang.Object apply()>
<org.apache.spark.api.java.AbstractJavaRDDLike: java.lang.String toDebugString()>
<org.apache.spark.rdd.DefaultPartitionCoalescer$$anonfun$throwBalls$9: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: org.apache.spark.broadcast.Broadcast broadcast(java.lang.Object,scala.reflect.ClassTag)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.broadcast.Broadcast broadcast(java.lang.Object)>
<org.apache.spark.ShuffleStatus: byte[] serializedMapStatus(org.apache.spark.broadcast.BroadcastManager,boolean,int)>
<org.apache.spark.MapOutputTracker: scala.Tuple2 serializeMapStatuses(org.apache.spark.scheduler.MapStatus[],org.apache.spark.broadcast.BroadcastManager,boolean,int)>
<org.apache.spark.api.python.PythonRDD$: org.apache.spark.broadcast.Broadcast readBroadcastFromFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD sequenceFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD newAPIHadoopFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD newAPIHadoopRDD(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD hadoopFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD hadoopRDD(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.SparkContext$$anonfun$newAPIHadoopRDD$1: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$checkpointFile$1: java.lang.Object apply()>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.broadcast.Broadcast readBroadcastFromFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String)>

<org.apache.spark.util.random.XORShiftRandom$: scala.collection.immutable.Map benchmark(int)>
<org.apache.spark.util.random.XORShiftRandom: scala.collection.immutable.Map benchmark(int)>



<org.apache.spark.rdd.RDD$$anonfun$coalesce$1$$anonfun$8: scala.collection.Iterator apply(int,scala.collection.Iterator)>
<org.apache.spark.rdd.RDD$$anonfun$coalesce$1$$anonfun$8: java.lang.Object apply(java.lang.Object,java.lang.Object)>


<org.apache.spark.util.random.XORShiftRandom: void <init>(long)>
<org.apache.spark.util.random.SamplingUtils: scala.Tuple2 reservoirSampleAndCount(scala.collection.Iterator,int,long,scala.reflect.ClassTag)>
<org.apache.spark.util.random.XORShiftRandom: scala.collection.immutable.Map benchmark(int)>
<org.apache.spark.RangePartitioner$$anonfun$13: java.lang.Object apply(java.lang.Object,java.lang.Object)>
<org.apache.spark.util.random.GapSamplingReplacement$: java.util.Random $lessinit$greater$default$2()>
<org.apache.spark.util.random.GapSampling$: java.util.Random $lessinit$greater$default$2()>
<org.apache.spark.util.random.BernoulliCellSampler: java.lang.Object clone()>
<org.apache.spark.util.random.BernoulliCellSampler: org.apache.spark.util.random.RandomSampler clone()>
<org.apache.spark.rdd.RDD$$anonfun$randomSampleWithRange$1: java.lang.Object apply(java.lang.Object,java.lang.Object)>
<org.apache.spark.util.random.StratifiedSamplingUtils$$anonfun$getBernoulliSamplingFunction$1: java.lang.Object apply(java.lang.Object,java.lang.Object)>
<org.apache.spark.util.random.StratifiedSamplingUtils$$anonfun$getPoissonSamplingFunction$2: java.lang.Object apply(java.lang.Object,java.lang.Object)>
<org.apache.spark.util.random.StratifiedSamplingUtils$$anonfun$getPoissonSamplingFunction$1: java.lang.Object apply(java.lang.Object,java.lang.Object)>
<org.apache.spark.util.random.StratifiedSamplingUtils$$anonfun$2: java.lang.Object apply(java.lang.Object,java.lang.Object)>
<org.apache.spark.util.random.GapSamplingReplacement: java.util.Random $lessinit$greater$default$2()>
<org.apache.spark.util.random.GapSampling: java.util.Random $lessinit$greater$default$2()>
<org.apache.spark.util.random.BernoulliSampler: java.lang.Object clone()>
<org.apache.spark.util.random.BernoulliSampler: org.apache.spark.util.random.RandomSampler clone()>
<org.apache.spark.rdd.RDD$$anonfun$sample$2: java.lang.Object apply()>
<org.apache.spark.util.random.PoissonSampler: java.lang.Object clone()>
<org.apache.spark.util.random.PoissonSampler: org.apache.spark.util.random.RandomSampler clone()>
<org.apache.spark.rdd.RDD$$anonfun$7: scala.collection.Iterator apply()>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1: scala.collection.GenTraversableOnce apply(scala.Tuple2)>
<org.apache.spark.rdd.MapPartitionsRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1: scala.collection.Iterator apply(java.lang.Object)>
<org.apache.spark.rdd.ZippedPartitionsRDD4: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.UnionRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.api.python.PythonRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1: scala.collection.Iterator apply(org.apache.spark.Partition)>
<org.apache.spark.rdd.ZippedWithIndexRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.CartesianRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.scheduler.ResultTask: java.lang.Object runTask(org.apache.spark.TaskContext)>
<org.apache.spark.api.r.BaseRRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.ZippedPartitionsRDD3: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.api.python.PairwiseRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.ZippedPartitionsRDD2: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.PartitionPruningRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.NewHadoopRDD$NewHadoopMapPartitionsWithSplitRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.api.java.JavaRDDLike$class: java.util.Iterator iterator(org.apache.spark.api.java.JavaRDDLike,org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.HadoopRDD$HadoopMapPartitionsWithSplitRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.RDD$$anonfun$7: java.lang.Object apply()>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.ShuffleMapTask: java.lang.Object runTask(org.apache.spark.TaskContext)>
<org.apache.spark.api.java.AbstractJavaRDDLike: java.util.Iterator iterator(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.spark_project.jetty.server.session.JDBCSessionManager: org.spark_project.jetty.server.session.JDBCSessionManager$Session getSession(java.lang.String)>
<org.spark_project.jetty.server.session.JDBCSessionManager: org.spark_project.jetty.server.session.AbstractSession getSession(java.lang.String)>
<org.spark_project.jetty.io.ByteArrayEndPoint: java.lang.String takeOutputString(java.nio.charset.Charset)>
<org.spark_project.jetty.http.ResourceHttpContent: java.nio.ByteBuffer getDirectBuffer()>
<org.spark_project.jetty.http.ResourceHttpContent: java.nio.ByteBuffer getIndirectBuffer()>
<org.spark_project.jetty.io.WriteFlusher: boolean onFail(java.lang.Throwable)>
<org.spark_project.jetty.client.util.DeferredContentProvider: boolean offer(java.nio.ByteBuffer,org.spark_project.jetty.util.Callback)>
<org.spark_project.jetty.server.LocalConnector: org.spark_project.jetty.server.LocalConnector$LocalEndPoint executeRequest(java.lang.String)>
<org.spark_project.jetty.io.ByteArrayEndPoint: java.lang.String takeOutputString()>
<org.spark_project.jetty.server.ResourceCache$CachedHttpContent: java.nio.ByteBuffer getIndirectBuffer()>
<org.spark_project.jetty.server.ResourceCache$CachedHttpContent: java.nio.ByteBuffer getDirectBuffer()>
<org.spark_project.jetty.io.AbstractEndPoint: boolean tryFillInterested(org.spark_project.jetty.util.Callback)>
<org.spark_project.jetty.client.util.DeferredContentProvider: boolean offer(java.nio.ByteBuffer)>
<org.spark_project.jetty.server.LocalConnector: java.lang.String getResponses(java.lang.String,long,java.util.concurrent.TimeUnit)>
<org.spark_project.jetty.server.LocalConnector: java.nio.ByteBuffer getResponses(java.nio.ByteBuffer)>
<org.spark_project.jetty.server.LocalConnector: java.nio.ByteBuffer getResponse(java.nio.ByteBuffer)>
<org.spark_project.jetty.server.LocalConnector: java.lang.String getResponse(java.lang.String)>
<org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anonfun$apply$mcV$sp$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$2$$anonfun$run$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$2$$anonfun$run$4: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$cancelLastRegistrationRetry$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anonfun$apply$mcV$sp$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$onStop$1: java.lang.Object apply(java.lang.Object)>
<org.spark_project.jetty.client.HttpClient: org.spark_project.jetty.client.api.ContentResponse GET(java.lang.String)>
<org.spark_project.jetty.client.HttpClient: org.spark_project.jetty.client.api.ContentResponse FORM(java.lang.String,org.spark_project.jetty.util.Fields)>
<org.apache.spark.util.Utils: java.lang.Object tryWithResource(scala.Function0,scala.Function1)>
<org.apache.spark.storage.DiskBlockData: org.apache.spark.util.io.ChunkedByteBuffer toChunkedByteBuffer(scala.Function1)>
<org.apache.spark.storage.DiskBlockData: java.nio.ByteBuffer toByteBuffer()>
<org.spark_project.jetty.server.LocalConnector: java.lang.String getResponses(java.lang.String)>
<org.apache.spark.deploy.history.FsHistoryProvider: scala.Option getAppUI(java.lang.String,scala.Option)>
<org.apache.spark.status.api.v1.StagesResource$$anonfun$taskSummary$1: org.apache.spark.status.api.v1.TaskMetricDistributions apply(org.apache.spark.ui.SparkUI)>
<org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$close$2: org.apache.spark.storage.FileSegment apply()>
<org.spark_project.jetty.server.Request: java.lang.String getParameter(java.lang.String)>
<org.spark_project.jetty.server.Request: java.util.Map getParameterMap()>
<org.spark_project.jetty.server.Request: java.util.Enumeration getParameterNames()>
<org.spark_project.jetty.server.Request: java.lang.String[] getParameterValues(java.lang.String)>
<org.spark_project.jetty.client.DuplexConnectionPool: org.spark_project.jetty.client.api.Connection acquire()>
<org.spark_project.jetty.client.DuplexConnectionPool: boolean release(org.spark_project.jetty.client.api.Connection)>
<org.apache.spark.rpc.netty.NettyRpcEndpointRef: scala.concurrent.Future ask(java.lang.Object,org.apache.spark.rpc.RpcTimeout,scala.reflect.ClassTag)>
<org.apache.spark.status.api.v1.StagesResource$$anonfun$taskSummary$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager$$anonfun$26: java.lang.Object apply()>
<org.apache.spark.rpc.netty.NettyRpcEnv: java.nio.ByteBuffer serialize(java.lang.Object)>
<org.apache.spark.serializer.SerializerManager: org.apache.spark.util.io.ChunkedByteBuffer dataSerialize(org.apache.spark.storage.BlockId,scala.collection.Iterator,scala.reflect.ClassTag)>
<org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$close$2: java.lang.Object apply()>
<org.spark_project.jetty.client.PoolingHttpDestination: org.spark_project.jetty.client.api.Connection acquire()>
<org.apache.spark.rpc.RpcEnv$: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,int,boolean)>
<org.spark_project.jetty.client.HttpChannel: boolean abortResponse(org.spark_project.jetty.client.HttpExchange,java.lang.Throwable)>
<org.apache.spark.util.Utils$$anonfun$copyStream$1: long apply()>
<org.apache.spark.ui.WebUI$$anonfun$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$foldByKey$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1: java.lang.Object apply()>
<org.apache.spark.scheduler.TaskSetManager$$anonfun$resourceOffer$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.RDD$$anonfun$fold$1: java.lang.Object apply()>
<org.apache.spark.util.Utils: java.lang.Object clone(java.lang.Object,org.apache.spark.serializer.SerializerInstance,scala.reflect.ClassTag)>
<org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$aggregate$1: java.lang.Object apply()>
<org.apache.spark.scheduler.Task: byte[] $lessinit$greater$default$5()>
<org.apache.spark.storage.BlockManager$$anonfun$getLocalBytes$2: org.apache.spark.storage.BlockData apply(org.apache.spark.storage.BlockInfo)>
<org.apache.spark.SparkContext: org.apache.spark.SparkContext getOrCreate(org.apache.spark.SparkConf)>
<org.spark_project.jetty.server.Request: java.lang.String getRemoteUser()>
<org.apache.spark.unsafe.map.BytesToBytesMap: long spill(long,org.apache.spark.memory.MemoryConsumer)>
<org.apache.spark.ui.jobs.AllJobsPage: scala.collection.Seq render(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.deploy.worker.ui.WorkerWebUI$$anonfun$initialize$1: java.lang.String apply(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$onStart$2: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$5: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.HeartbeatReceiver$$anonfun$removeExecutor$1: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.HeartbeatReceiver$$anonfun$addExecutor$1: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.scheduler.OutputCommitCoordinator: boolean canCommit(int,int,int)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockFromWorkers$1: java.lang.Object apply(org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeShuffle$1: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.deploy.client.StandaloneAppClient: scala.concurrent.Future requestTotalExecutors(int)>
<org.apache.spark.deploy.client.StandaloneAppClient: scala.concurrent.Future killExecutors(scala.collection.Seq)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockManager$2: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBroadcast$1: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$getMatchingBlockIds$1: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.HeartbeatReceiver$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.scheduler.DAGScheduler: boolean executorHeartbeatReceived(java.lang.String,scala.Tuple4[],org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.rpc.RpcEndpointRef: java.lang.Object askSync(java.lang.Object,scala.reflect.ClassTag)>
<org.apache.spark.rpc.RpcEnv: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,int,boolean)>
<org.apache.spark.rpc.RpcEnv$: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)>
<org.apache.spark.scheduler.TaskDescription: java.nio.ByteBuffer encode(org.apache.spark.scheduler.TaskDescription)>
<org.apache.spark.util.Utils$$anonfun$copyStream$1: java.lang.Object apply()>
<org.apache.spark.deploy.history.ApplicationCache$$anonfun$1: java.lang.Object apply()>
<org.apache.spark.storage.memory.MemoryStore: scala.util.Either putIteratorAsValues(org.apache.spark.storage.BlockId,scala.collection.Iterator,scala.reflect.ClassTag)>
<org.apache.spark.storage.memory.MemoryStore: scala.util.Either putIteratorAsBytes(org.apache.spark.storage.BlockId,scala.collection.Iterator,scala.reflect.ClassTag,org.apache.spark.memory.MemoryMode)>
<org.apache.spark.storage.BlockManager$$anonfun$getLocalBytes$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: org.apache.spark.SparkContext getOrCreate()>
<org.apache.spark.api.r.RRDD: org.apache.spark.api.java.JavaSparkContext createSparkContext(java.lang.String,java.lang.String,java.lang.String,java.lang.String[],java.util.Map,java.util.Map)>
<org.apache.spark.util.collection.ExternalSorter: boolean forceSpill()>
<org.apache.spark.memory.MemoryConsumer: long acquireMemory(long)>
<org.apache.spark.deploy.worker.ui.WorkerWebUI$$anonfun$initialize$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$onStart$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$blockStatus$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anonfun$removeExecutor$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anonfun$addExecutor$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockFromWorkers$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeShuffle$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend: scala.concurrent.Future doRequestTotalExecutors(int)>
<org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend: scala.concurrent.Future doKillExecutors(scala.collection.Seq)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockManager$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBroadcast$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$getMatchingBlockIds$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.TaskSchedulerImpl: boolean executorHeartbeatReceived(java.lang.String,scala.Tuple2[],org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.storage.BlockManagerMaster: org.apache.spark.storage.BlockManagerId registerBlockManager(org.apache.spark.storage.BlockManagerId,long,long,org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.storage.BlockManagerMaster: boolean updateBlockInfo(org.apache.spark.storage.BlockManagerId,org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel,long,long)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.Seq getLocations(org.apache.spark.storage.BlockId)>
<org.apache.spark.storage.BlockManagerMaster: scala.Option getLocationsAndStatus(org.apache.spark.storage.BlockId)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.IndexedSeq getLocations(org.apache.spark.storage.BlockId[])>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.Seq getPeers(org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.storage.BlockManagerMaster: scala.Option getExecutorEndpointRef(java.lang.String)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.immutable.Map getMemoryStatus()>
<org.apache.spark.storage.BlockManagerMaster: org.apache.spark.storage.StorageStatus[] getStorageStatus()>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.immutable.Map getBlockStatus(org.apache.spark.storage.BlockId,boolean)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.Seq getMatchingBlockIds(scala.Function1,boolean)>
<org.apache.spark.storage.BlockManagerMaster: boolean hasCachedBlocks(java.lang.String)>
<org.apache.spark.deploy.master.ui.MasterWebUI: scala.Option idToUiAddress(java.lang.String)>
<org.apache.spark.MapOutputTracker: java.lang.Object askTracker(java.lang.Object,scala.reflect.ClassTag)>
<org.apache.spark.deploy.master.ui.MasterPage: org.apache.spark.deploy.DeployMessages$MasterStateResponse getMasterState()>
<org.apache.spark.deploy.worker.ui.WorkerPage: org.json4s.JsonAST$JValue renderJson(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.deploy.worker.ui.WorkerPage: scala.collection.Seq render(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.SparkEnv$: org.apache.spark.SparkEnv createDriverEnv(org.apache.spark.SparkConf,boolean,org.apache.spark.scheduler.LiveListenerBus,int,scala.Option)>
<org.apache.spark.SparkEnv$: org.apache.spark.SparkEnv createExecutorEnv(org.apache.spark.SparkConf,java.lang.String,java.lang.String,int,scala.Option,boolean)>
<org.apache.spark.deploy.worker.Worker$: org.apache.spark.rpc.RpcEnv startRpcEnvAndEndpoint(java.lang.String,int,int,int,int,java.lang.String[],java.lang.String,scala.Option,org.apache.spark.SparkConf)>
<org.apache.spark.rpc.RpcEnv: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)>
<org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager: scala.util.Either getOrElseUpdate(org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel,scala.reflect.ClassTag,scala.Function0)>
<org.apache.spark.storage.BlockManager: scala.Option getSingle(org.apache.spark.storage.BlockId,scala.reflect.ClassTag)>
<org.apache.spark.rdd.BlockRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.executor.TaskMetrics$$anonfun$nonZeroInternalAccums$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager$$anonfun$doPutBytes$1: scala.Option apply(org.apache.spark.storage.BlockInfo)>
<org.apache.spark.status.api.v1.BaseAppResource$$anonfun$withUI$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: boolean killTaskAttempt(long,boolean,java.lang.String)>
<org.apache.spark.storage.BlockManager: boolean org$apache$spark$storage$BlockManager$$tryToReportBlockStatus(org.apache.spark.storage.BlockId,org.apache.spark.storage.BlockStatus,long)>
<org.apache.spark.scheduler.DAGScheduler: scala.collection.IndexedSeq getCacheLocs(org.apache.spark.rdd.RDD)>
<org.apache.spark.storage.BlockManager: scala.collection.Seq[] org$apache$spark$storage$BlockManager$$getLocationBlockIds(org.apache.spark.storage.BlockId[])>
<org.apache.spark.SparkContext: scala.collection.Map getExecutorMemoryStatus()>
<org.apache.spark.SparkContext: org.apache.spark.storage.StorageStatus[] getExecutorStorageStatus()>
<org.apache.spark.storage.BlockManagerSource$$anonfun$5: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$2: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$6: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$4: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$10: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$3: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$1: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$7: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$8: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$9: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.deploy.master.ui.MasterWebUI$$anonfun$3: scala.Option apply(java.lang.String)>
<org.apache.spark.deploy.LocalSparkCluster: java.lang.String[] start()>
<org.apache.spark.deploy.master.Master: scala.Tuple3 startRpcEnvAndEndpoint(java.lang.String,int,int,org.apache.spark.SparkConf)>
<org.apache.spark.deploy.master.ui.MasterPage: org.json4s.JsonAST$JValue renderJson(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.deploy.master.ui.MasterPage: scala.collection.Seq render(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.SparkContext: org.apache.spark.SparkEnv createSparkEnv(org.apache.spark.SparkConf,boolean,org.apache.spark.scheduler.LiveListenerBus)>
<org.apache.spark.deploy.worker.Worker: org.apache.spark.rpc.RpcEnv startRpcEnvAndEndpoint(java.lang.String,int,int,int,int,java.lang.String[],java.lang.String,scala.Option,org.apache.spark.SparkConf)>
<org.apache.spark.deploy.LocalSparkCluster$$anonfun$start$2: scala.collection.mutable.ArrayBuffer apply(int)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$client$StandaloneAppClient$ClientEndpoint$$askAndReplyAsync$1: java.lang.Object applyOrElse(java.lang.Object,scala.Function1)>
<org.apache.spark.SparkContext: org.apache.spark.partial.PartialResult runApproximateJob(org.apache.spark.rdd.RDD,scala.Function2,org.apache.spark.partial.ApproximateEvaluator,long)>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$map$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$reduce$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1: scala.collection.mutable.Map apply()>
<org.apache.spark.rdd.RDD$$anonfun$filter$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$keyBy$1: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.SparkContext$$anonfun$sequenceFile$3: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$treeReduce$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreach$1: scala.runtime.BoxedUnit[] apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$flatMapValues$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$collect$2: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1: scala.runtime.BoxedUnit[] apply()>
<org.apache.spark.rdd.RDD$$anonfun$groupBy$3: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$flatMap$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.storage.BlockManager$$anonfun$doPutBytes$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$3: scala.collection.Seq apply(scala.collection.Set)>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$$anonfun$13: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.DAGScheduler: scala.collection.Seq org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal(org.apache.spark.rdd.RDD,int,scala.collection.mutable.HashSet)>
<org.apache.spark.storage.BlockManager: scala.collection.immutable.Map blockIdsToHosts(org.apache.spark.storage.BlockId[],org.apache.spark.SparkEnv,org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$6: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$4: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$10: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$7: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$8: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$9: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.master.ui.MasterWebUI$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.rest.StatusRequestServlet$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.rest.KillRequestServlet$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.ui.WebUI$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.LocalSparkCluster$$anonfun$start$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulable accumulable(java.lang.Object,java.lang.String,org.apache.spark.AccumulableParam)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator intAccumulator(int)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator doubleAccumulator(double)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(java.lang.Object,org.apache.spark.AccumulatorParam)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator intAccumulator(int,java.lang.String)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator doubleAccumulator(double,java.lang.String)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(java.lang.Object,java.lang.String,org.apache.spark.AccumulatorParam)>
<org.apache.spark.SparkContext: java.lang.Object runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.reflect.ClassTag)>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$lookup$1: scala.collection.Seq apply()>
<org.apache.spark.rdd.RDD$$anonfun$toLocalIterator$1: java.lang.Object org$apache$spark$rdd$RDD$$anonfun$$collectPartition$1(int)>
<org.apache.spark.rdd.RDD$$anonfun$take$1: java.lang.Object apply()>
<org.apache.spark.api.java.JavaRDDLike$class: java.util.List[] collectPartitions(org.apache.spark.api.java.JavaRDDLike,int[])>
<org.apache.spark.api.python.PythonRDD$: int runJob(org.apache.spark.SparkContext,org.apache.spark.api.java.JavaRDD,java.util.ArrayList)>
<org.apache.spark.rdd.RDD$$anonfun$countByValueApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sumApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.rdd.RDD$$anonfun$countApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$meanApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.ComplexFutureAction$$anon$1: org.apache.spark.FutureAction submitJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.Function2,scala.Function0)>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachPartitionAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$collectAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$countAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$map$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$zipPartitions$5: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$filter$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$keyBy$1: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$sequenceFile$3: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreach$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$pipe$3: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$zipPartitions$3: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$flatMapValues$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$collect$2: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$zipPartitions$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKeyWithClassTag$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$groupBy$3: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$flatMap$1: java.lang.Object apply()>
<org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2: java.lang.Object apply()>
<org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4: java.lang.Object apply()>
<org.apache.spark.SparkContext: boolean killExecutor(java.lang.String)>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend: boolean killExecutor(java.lang.String)>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.LocalRDDCheckpointData$$anonfun$1: boolean apply(int)>
<org.apache.spark.scheduler.DAGScheduler: scala.collection.Seq getPreferredLocs(org.apache.spark.rdd.RDD,int)>
<org.apache.spark.api.java.JavaPairRDD: org.apache.spark.api.java.JavaPairRDD unpersist()>
<org.apache.spark.api.java.JavaPairRDD: org.apache.spark.api.java.JavaPairRDD unpersist(boolean)>
<org.apache.spark.api.java.JavaDoubleRDD: org.apache.spark.api.java.JavaDoubleRDD unpersist()>
<org.apache.spark.api.java.JavaDoubleRDD: org.apache.spark.api.java.JavaDoubleRDD unpersist(boolean)>
<org.apache.spark.api.java.JavaRDD: org.apache.spark.api.java.JavaRDD unpersist()>
<org.apache.spark.api.java.JavaRDD: org.apache.spark.api.java.JavaRDD unpersist(boolean)>
<org.apache.spark.SparkContext: org.apache.spark.storage.RDDInfo[] getRDDStorageInfo()>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulable accumulable(java.lang.Object,org.apache.spark.AccumulableParam)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(int)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(double)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(int,java.lang.String)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(double,java.lang.String)>
<org.apache.spark.SparkContext: java.lang.Object runJob(org.apache.spark.rdd.RDD,scala.Function2,scala.reflect.ClassTag)>
<org.apache.spark.rdd.RDD: long count()>
<org.apache.spark.rdd.RDD$$anonfun$collectPartitions$1: java.lang.Object[] apply()>
<org.apache.spark.rdd.RDD$$anonfun$collect$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$lookup$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$toLocalIterator$1$$anonfun$apply$30: scala.collection.mutable.ArrayOps apply(int)>
<org.apache.spark.api.java.AbstractJavaRDDLike: java.util.List[] collectPartitions(int[])>
<org.apache.spark.api.python.PythonRDD: int runJob(org.apache.spark.SparkContext,org.apache.spark.api.java.JavaRDD,java.util.ArrayList)>
<org.apache.spark.rdd.RDD$$anonfun$countByValueApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sumApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$countApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$meanApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachPartitionAsync$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$collectAsync$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$countAsync$1: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$hadoopFile$1: java.lang.Object apply()>
<org.apache.spark.rdd.LocalRDDCheckpointData$$anonfun$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: scala.collection.Seq getPreferredLocs(org.apache.spark.rdd.RDD,int)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobSubmitted$5: java.lang.String apply()>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleMapStageSubmitted$5: java.lang.String apply()>
<org.apache.spark.rdd.BlockRDD: scala.collection.Seq getPreferredLocations(org.apache.spark.Partition)>
<org.apache.spark.rdd.BlockRDD: scala.collection.immutable.Map getBlockIdLocations()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$7: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$1: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$5: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$6: long apply$mcJ$sp()>
<org.apache.spark.rdd.RDD$$anonfun$takeSample$1: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$3: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$4: long apply$mcJ$sp()>
<org.apache.spark.api.java.JavaRDDLike$class: long count(org.apache.spark.api.java.JavaRDDLike)>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$2: long apply$mcJ$sp()>
<org.apache.spark.rdd.RDD$$anonfun$collectPartitions$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$toLocalIterator$1$$anonfun$apply$30: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.RDD$$anonfun$zipWithIndex$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$org$apache$spark$rdd$AsyncRDDActions$$anonfun$$continue$1$1: scala.concurrent.Future apply(scala.Unit$)>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$11: scala.concurrent.Future apply(org.apache.spark.JobSubmitter)>
<org.apache.spark.SparkContext$$anonfun$hadoopRDD$1: java.lang.Object apply()>
<org.apache.spark.rdd.DefaultPartitionCoalescer: scala.collection.Seq currPrefLocs(org.apache.spark.Partition,org.apache.spark.rdd.RDD)>
<org.apache.spark.rdd.PartitionerAwareUnionRDD: scala.collection.Seq org$apache$spark$rdd$PartitionerAwareUnionRDD$$currPrefLocs(org.apache.spark.rdd.RDD,org.apache.spark.Partition)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$15: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$16: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobSubmitted$5: java.lang.Object apply()>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleMapStageSubmitted$5: java.lang.Object apply()>
<org.apache.spark.rdd.RDD: java.lang.String toDebugString()>
<org.apache.spark.rdd.RDD$$anonfun$38: scala.collection.Seq apply(org.apache.spark.Dependency)>
<org.apache.spark.rdd.ReliableRDDCheckpointData: org.apache.spark.rdd.CheckpointRDD doCheckpoint()>
<org.apache.spark.rdd.ReliableCheckpointRDD: org.apache.spark.rdd.ReliableCheckpointRDD writeRDDToCheckpointDirectory(org.apache.spark.rdd.RDD,java.lang.String,int,scala.reflect.ClassTag)>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$7: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$1: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$5: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$6: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$3: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$4: long apply()>
<org.apache.spark.api.java.AbstractJavaRDDLike: long count()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$2: long apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$org$apache$spark$rdd$AsyncRDDActions$$anonfun$$continue$1$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$11: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager: boolean putBytes(org.apache.spark.storage.BlockId,org.apache.spark.util.io.ChunkedByteBuffer,org.apache.spark.storage.StorageLevel,boolean,scala.reflect.ClassTag)>
<org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1: boolean apply()>
<org.apache.spark.rdd.CoalescedRDDPartition$$anonfun$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations$$anonfun$getAllPrefLocs$2: java.lang.Object apply(org.apache.spark.Partition)>
<org.apache.spark.rdd.DefaultPartitionCoalescer: org.apache.spark.rdd.PartitionGroup pickBin(org.apache.spark.Partition,org.apache.spark.rdd.RDD,double,org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations)>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$5: scala.collection.Seq apply(scala.Tuple2)>
<org.apache.spark.rdd.RDD$$anonfun$preferredLocations$2: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$runJob$2: java.lang.String apply()>
<org.apache.spark.api.java.JavaRDDLike$class: java.lang.String toDebugString(org.apache.spark.api.java.JavaRDDLike)>
<org.apache.spark.rdd.RDD$$anonfun$38: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$7: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$1: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$5: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$6: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$3: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$4: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$2: java.lang.Object apply()>
<org.apache.spark.storage.BlockManager: boolean putSingle(org.apache.spark.storage.BlockId,java.lang.Object,org.apache.spark.storage.StorageLevel,boolean,scala.reflect.ClassTag)>
<org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1: java.lang.Object apply()>
<org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations$$anonfun$getAllPrefLocs$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.DefaultPartitionCoalescer$$anonfun$throwBalls$9: scala.collection.mutable.ArrayBuffer apply(org.apache.spark.Partition)>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext$$anonfun$runJob$2: java.lang.Object apply()>
<org.apache.spark.api.java.AbstractJavaRDDLike: java.lang.String toDebugString()>
<org.apache.spark.rdd.DefaultPartitionCoalescer$$anonfun$throwBalls$9: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: org.apache.spark.broadcast.Broadcast broadcast(java.lang.Object,scala.reflect.ClassTag)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.broadcast.Broadcast broadcast(java.lang.Object)>
<org.apache.spark.ShuffleStatus: byte[] serializedMapStatus(org.apache.spark.broadcast.BroadcastManager,boolean,int)>
<org.apache.spark.MapOutputTracker: scala.Tuple2 serializeMapStatuses(org.apache.spark.scheduler.MapStatus[],org.apache.spark.broadcast.BroadcastManager,boolean,int)>
<org.apache.spark.api.python.PythonRDD$: org.apache.spark.broadcast.Broadcast readBroadcastFromFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD sequenceFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD newAPIHadoopFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD newAPIHadoopRDD(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD hadoopFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD hadoopRDD(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.SparkContext$$anonfun$newAPIHadoopRDD$1: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$checkpointFile$1: java.lang.Object apply()>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.broadcast.Broadcast readBroadcastFromFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String)>

<org.apache.spark.rdd.PartitionwiseSampledRDD: org.apache.spark.Partition[] getPartitions()>
<org.apache.spark.rdd.RDD$$anonfun$partitions$2: java.lang.Object apply()>

<org.spark_project.jetty.server.session.AbstractSessionIdManager: void initRandom()>

<org.apache.spark.rdd.RDD$$anonfun$takeSample$1: java.lang.Object apply()>

<org.apache.spark.util.SizeEstimator$: long estimate(java.lang.Object)>
<org.apache.spark.util.SizeEstimator: long estimate(java.lang.Object)>
<org.apache.spark.storage.memory.MemoryStore: scala.util.Either putIteratorAsValues(org.apache.spark.storage.BlockId,scala.collection.Iterator,scala.reflect.ClassTag)>
<org.apache.spark.storage.memory.MemoryStore: scala.util.Either putIteratorAsBytes(org.apache.spark.storage.BlockId,scala.collection.Iterator,scala.reflect.ClassTag,org.apache.spark.memory.MemoryMode)>
<org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1: scala.Option apply(org.apache.spark.storage.BlockInfo)>
<org.apache.spark.storage.BlockManager$$anonfun$doPutBytes$1: scala.Option apply(org.apache.spark.storage.BlockInfo)>
<org.apache.spark.util.collection.PrimitiveVector: org.apache.spark.util.collection.PrimitiveVector trim$mcD$sp()>
<org.apache.spark.util.collection.PrimitiveVector: org.apache.spark.util.collection.PrimitiveVector trim$mcI$sp()>
<org.apache.spark.util.collection.PrimitiveVector: org.apache.spark.util.collection.PrimitiveVector trim$mcJ$sp()>
<org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager$$anonfun$doPutBytes$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKeyWithClassTag$1$$anonfun$apply$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2: java.lang.Object apply()>
<org.apache.spark.storage.BlockManager: scala.Option get(org.apache.spark.storage.BlockId,scala.reflect.ClassTag)>
<org.apache.spark.storage.BlockManager: scala.util.Either getOrElseUpdate(org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel,scala.reflect.ClassTag,scala.Function0)>
<org.apache.spark.scheduler.ShuffleMapTask: java.lang.Object runTask(org.apache.spark.TaskContext)>
<org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager: scala.Option getSingle(org.apache.spark.storage.BlockId,scala.reflect.ClassTag)>
<org.apache.spark.rdd.BlockRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.memory.MemoryConsumer: long acquireMemory(long)>
<org.apache.spark.rdd.RDD$$anonfun$7: scala.collection.Iterator apply()>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1: scala.collection.GenTraversableOnce apply(scala.Tuple2)>
<org.apache.spark.rdd.MapPartitionsRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1: scala.collection.Iterator apply(java.lang.Object)>
<org.apache.spark.rdd.ZippedPartitionsRDD4: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.UnionRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.api.python.PythonRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1: scala.collection.Iterator apply(org.apache.spark.Partition)>
<org.apache.spark.rdd.ZippedWithIndexRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.CartesianRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.scheduler.ResultTask: java.lang.Object runTask(org.apache.spark.TaskContext)>
<org.apache.spark.api.r.BaseRRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.ZippedPartitionsRDD3: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.api.python.PairwiseRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.ZippedPartitionsRDD2: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.PartitionPruningRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.NewHadoopRDD$NewHadoopMapPartitionsWithSplitRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.PartitionwiseSampledRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.api.java.JavaRDDLike$class: java.util.Iterator iterator(org.apache.spark.api.java.JavaRDDLike,org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.HadoopRDD$HadoopMapPartitionsWithSplitRDD: scala.collection.Iterator compute(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.apache.spark.rdd.RDD$$anonfun$7: java.lang.Object apply()>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.api.java.AbstractJavaRDDLike: java.util.Iterator iterator(org.apache.spark.Partition,org.apache.spark.TaskContext)>
<org.spark_project.jetty.server.session.JDBCSessionManager: org.spark_project.jetty.server.session.JDBCSessionManager$Session getSession(java.lang.String)>
<org.spark_project.jetty.server.session.JDBCSessionManager: org.spark_project.jetty.server.session.AbstractSession getSession(java.lang.String)>
<org.spark_project.jetty.io.ByteArrayEndPoint: java.lang.String takeOutputString(java.nio.charset.Charset)>
<org.spark_project.jetty.http.ResourceHttpContent: java.nio.ByteBuffer getDirectBuffer()>
<org.spark_project.jetty.http.ResourceHttpContent: java.nio.ByteBuffer getIndirectBuffer()>
<org.spark_project.jetty.io.WriteFlusher: boolean onFail(java.lang.Throwable)>
<org.spark_project.jetty.client.util.DeferredContentProvider: boolean offer(java.nio.ByteBuffer,org.spark_project.jetty.util.Callback)>
<org.spark_project.jetty.server.LocalConnector: org.spark_project.jetty.server.LocalConnector$LocalEndPoint executeRequest(java.lang.String)>
<org.spark_project.jetty.io.ByteArrayEndPoint: java.lang.String takeOutputString()>
<org.spark_project.jetty.server.ResourceCache$CachedHttpContent: java.nio.ByteBuffer getIndirectBuffer()>
<org.spark_project.jetty.server.ResourceCache$CachedHttpContent: java.nio.ByteBuffer getDirectBuffer()>
<org.spark_project.jetty.io.AbstractEndPoint: boolean tryFillInterested(org.spark_project.jetty.util.Callback)>
<org.spark_project.jetty.client.util.DeferredContentProvider: boolean offer(java.nio.ByteBuffer)>
<org.spark_project.jetty.server.LocalConnector: java.lang.String getResponses(java.lang.String,long,java.util.concurrent.TimeUnit)>
<org.spark_project.jetty.server.LocalConnector: java.nio.ByteBuffer getResponses(java.nio.ByteBuffer)>
<org.spark_project.jetty.server.LocalConnector: java.nio.ByteBuffer getResponse(java.nio.ByteBuffer)>
<org.spark_project.jetty.server.LocalConnector: java.lang.String getResponse(java.lang.String)>
<org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anonfun$apply$mcV$sp$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$2$$anonfun$run$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$2$$anonfun$run$4: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$cancelLastRegistrationRetry$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anonfun$apply$mcV$sp$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$onStop$1: java.lang.Object apply(java.lang.Object)>
<org.spark_project.jetty.client.HttpClient: org.spark_project.jetty.client.api.ContentResponse GET(java.lang.String)>
<org.spark_project.jetty.client.HttpClient: org.spark_project.jetty.client.api.ContentResponse FORM(java.lang.String,org.spark_project.jetty.util.Fields)>
<org.apache.spark.util.Utils: java.lang.Object tryWithResource(scala.Function0,scala.Function1)>
<org.apache.spark.storage.DiskBlockData: org.apache.spark.util.io.ChunkedByteBuffer toChunkedByteBuffer(scala.Function1)>
<org.apache.spark.storage.DiskBlockData: java.nio.ByteBuffer toByteBuffer()>
<org.spark_project.jetty.server.LocalConnector: java.lang.String getResponses(java.lang.String)>
<org.apache.spark.deploy.history.FsHistoryProvider: scala.Option getAppUI(java.lang.String,scala.Option)>
<org.apache.spark.status.api.v1.StagesResource$$anonfun$taskSummary$1: org.apache.spark.status.api.v1.TaskMetricDistributions apply(org.apache.spark.ui.SparkUI)>
<org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$close$2: org.apache.spark.storage.FileSegment apply()>
<org.spark_project.jetty.server.Request: java.lang.String getParameter(java.lang.String)>
<org.spark_project.jetty.server.Request: java.util.Map getParameterMap()>
<org.spark_project.jetty.server.Request: java.util.Enumeration getParameterNames()>
<org.spark_project.jetty.server.Request: java.lang.String[] getParameterValues(java.lang.String)>
<org.spark_project.jetty.client.DuplexConnectionPool: org.spark_project.jetty.client.api.Connection acquire()>
<org.spark_project.jetty.client.DuplexConnectionPool: boolean release(org.spark_project.jetty.client.api.Connection)>
<org.apache.spark.rpc.netty.NettyRpcEndpointRef: scala.concurrent.Future ask(java.lang.Object,org.apache.spark.rpc.RpcTimeout,scala.reflect.ClassTag)>
<org.apache.spark.status.api.v1.StagesResource$$anonfun$taskSummary$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager$$anonfun$26: java.lang.Object apply()>
<org.apache.spark.rpc.netty.NettyRpcEnv: java.nio.ByteBuffer serialize(java.lang.Object)>
<org.apache.spark.serializer.SerializerManager: org.apache.spark.util.io.ChunkedByteBuffer dataSerialize(org.apache.spark.storage.BlockId,scala.collection.Iterator,scala.reflect.ClassTag)>
<org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$close$2: java.lang.Object apply()>
<org.spark_project.jetty.client.PoolingHttpDestination: org.spark_project.jetty.client.api.Connection acquire()>
<org.apache.spark.rpc.RpcEnv$: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,int,boolean)>
<org.spark_project.jetty.client.HttpChannel: boolean abortResponse(org.spark_project.jetty.client.HttpExchange,java.lang.Throwable)>
<org.apache.spark.util.Utils$$anonfun$copyStream$1: long apply()>
<org.apache.spark.ui.WebUI$$anonfun$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$foldByKey$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1: java.lang.Object apply()>
<org.apache.spark.scheduler.TaskSetManager$$anonfun$resourceOffer$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.RDD$$anonfun$fold$1: java.lang.Object apply()>
<org.apache.spark.util.Utils: java.lang.Object clone(java.lang.Object,org.apache.spark.serializer.SerializerInstance,scala.reflect.ClassTag)>
<org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$aggregate$1: java.lang.Object apply()>
<org.apache.spark.scheduler.Task: byte[] $lessinit$greater$default$5()>
<org.apache.spark.storage.BlockManager$$anonfun$getLocalBytes$2: org.apache.spark.storage.BlockData apply(org.apache.spark.storage.BlockInfo)>
<org.apache.spark.SparkContext: org.apache.spark.SparkContext getOrCreate(org.apache.spark.SparkConf)>
<org.spark_project.jetty.server.Request: java.lang.String getRemoteUser()>
<org.apache.spark.unsafe.map.BytesToBytesMap: long spill(long,org.apache.spark.memory.MemoryConsumer)>
<org.apache.spark.ui.jobs.AllJobsPage: scala.collection.Seq render(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.deploy.worker.ui.WorkerWebUI$$anonfun$initialize$1: java.lang.String apply(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$onStart$2: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$5: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.HeartbeatReceiver$$anonfun$removeExecutor$1: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.HeartbeatReceiver$$anonfun$addExecutor$1: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.scheduler.OutputCommitCoordinator: boolean canCommit(int,int,int)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockFromWorkers$1: java.lang.Object apply(org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeShuffle$1: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.deploy.client.StandaloneAppClient: scala.concurrent.Future requestTotalExecutors(int)>
<org.apache.spark.deploy.client.StandaloneAppClient: scala.concurrent.Future killExecutors(scala.collection.Seq)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockManager$2: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBroadcast$1: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$getMatchingBlockIds$1: scala.concurrent.Future apply(org.apache.spark.storage.BlockManagerInfo)>
<org.apache.spark.HeartbeatReceiver$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1: scala.concurrent.Future apply(org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.scheduler.DAGScheduler: boolean executorHeartbeatReceived(java.lang.String,scala.Tuple4[],org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.rpc.RpcEndpointRef: java.lang.Object askSync(java.lang.Object,scala.reflect.ClassTag)>
<org.apache.spark.rpc.RpcEnv: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,int,boolean)>
<org.apache.spark.rpc.RpcEnv$: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)>
<org.apache.spark.scheduler.TaskDescription: java.nio.ByteBuffer encode(org.apache.spark.scheduler.TaskDescription)>
<org.apache.spark.util.Utils$$anonfun$copyStream$1: java.lang.Object apply()>
<org.apache.spark.deploy.history.ApplicationCache$$anonfun$1: java.lang.Object apply()>
<org.apache.spark.storage.BlockManager$$anonfun$getLocalBytes$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: org.apache.spark.SparkContext getOrCreate()>
<org.apache.spark.api.r.RRDD: org.apache.spark.api.java.JavaSparkContext createSparkContext(java.lang.String,java.lang.String,java.lang.String,java.lang.String[],java.util.Map,java.util.Map)>
<org.apache.spark.util.collection.ExternalSorter: boolean forceSpill()>
<org.apache.spark.deploy.worker.ui.WorkerWebUI$$anonfun$initialize$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$onStart$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$blockStatus$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anonfun$removeExecutor$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anonfun$addExecutor$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockFromWorkers$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeShuffle$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend: scala.concurrent.Future doRequestTotalExecutors(int)>
<org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend: scala.concurrent.Future doKillExecutors(scala.collection.Seq)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockManager$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBroadcast$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$getMatchingBlockIds$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.TaskSchedulerImpl: boolean executorHeartbeatReceived(java.lang.String,scala.Tuple2[],org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.storage.BlockManagerMaster: org.apache.spark.storage.BlockManagerId registerBlockManager(org.apache.spark.storage.BlockManagerId,long,long,org.apache.spark.rpc.RpcEndpointRef)>
<org.apache.spark.storage.BlockManagerMaster: boolean updateBlockInfo(org.apache.spark.storage.BlockManagerId,org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel,long,long)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.Seq getLocations(org.apache.spark.storage.BlockId)>
<org.apache.spark.storage.BlockManagerMaster: scala.Option getLocationsAndStatus(org.apache.spark.storage.BlockId)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.IndexedSeq getLocations(org.apache.spark.storage.BlockId[])>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.Seq getPeers(org.apache.spark.storage.BlockManagerId)>
<org.apache.spark.storage.BlockManagerMaster: scala.Option getExecutorEndpointRef(java.lang.String)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.immutable.Map getMemoryStatus()>
<org.apache.spark.storage.BlockManagerMaster: org.apache.spark.storage.StorageStatus[] getStorageStatus()>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.immutable.Map getBlockStatus(org.apache.spark.storage.BlockId,boolean)>
<org.apache.spark.storage.BlockManagerMaster: scala.collection.Seq getMatchingBlockIds(scala.Function1,boolean)>
<org.apache.spark.storage.BlockManagerMaster: boolean hasCachedBlocks(java.lang.String)>
<org.apache.spark.deploy.master.ui.MasterWebUI: scala.Option idToUiAddress(java.lang.String)>
<org.apache.spark.MapOutputTracker: java.lang.Object askTracker(java.lang.Object,scala.reflect.ClassTag)>
<org.apache.spark.deploy.master.ui.MasterPage: org.apache.spark.deploy.DeployMessages$MasterStateResponse getMasterState()>
<org.apache.spark.deploy.worker.ui.WorkerPage: org.json4s.JsonAST$JValue renderJson(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.deploy.worker.ui.WorkerPage: scala.collection.Seq render(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.SparkEnv$: org.apache.spark.SparkEnv createDriverEnv(org.apache.spark.SparkConf,boolean,org.apache.spark.scheduler.LiveListenerBus,int,scala.Option)>
<org.apache.spark.SparkEnv$: org.apache.spark.SparkEnv createExecutorEnv(org.apache.spark.SparkConf,java.lang.String,java.lang.String,int,scala.Option,boolean)>
<org.apache.spark.deploy.worker.Worker$: org.apache.spark.rpc.RpcEnv startRpcEnvAndEndpoint(java.lang.String,int,int,int,int,java.lang.String[],java.lang.String,scala.Option,org.apache.spark.SparkConf)>
<org.apache.spark.rpc.RpcEnv: org.apache.spark.rpc.RpcEnv create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)>
<org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.executor.TaskMetrics$$anonfun$nonZeroInternalAccums$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.status.api.v1.BaseAppResource$$anonfun$withUI$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: boolean killTaskAttempt(long,boolean,java.lang.String)>
<org.apache.spark.storage.BlockManager: boolean org$apache$spark$storage$BlockManager$$tryToReportBlockStatus(org.apache.spark.storage.BlockId,org.apache.spark.storage.BlockStatus,long)>
<org.apache.spark.scheduler.DAGScheduler: scala.collection.IndexedSeq getCacheLocs(org.apache.spark.rdd.RDD)>
<org.apache.spark.storage.BlockManager: scala.collection.Seq[] org$apache$spark$storage$BlockManager$$getLocationBlockIds(org.apache.spark.storage.BlockId[])>
<org.apache.spark.SparkContext: scala.collection.Map getExecutorMemoryStatus()>
<org.apache.spark.SparkContext: org.apache.spark.storage.StorageStatus[] getExecutorStorageStatus()>
<org.apache.spark.storage.BlockManagerSource$$anonfun$5: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$2: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$6: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$4: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$10: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$3: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$1: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$7: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$8: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$9: long apply(org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.deploy.master.ui.MasterWebUI$$anonfun$3: scala.Option apply(java.lang.String)>
<org.apache.spark.deploy.LocalSparkCluster: java.lang.String[] start()>
<org.apache.spark.deploy.master.Master: scala.Tuple3 startRpcEnvAndEndpoint(java.lang.String,int,int,org.apache.spark.SparkConf)>
<org.apache.spark.deploy.master.ui.MasterPage: org.json4s.JsonAST$JValue renderJson(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.deploy.master.ui.MasterPage: scala.collection.Seq render(javax.servlet.http.HttpServletRequest)>
<org.apache.spark.SparkContext: org.apache.spark.SparkEnv createSparkEnv(org.apache.spark.SparkConf,boolean,org.apache.spark.scheduler.LiveListenerBus)>
<org.apache.spark.deploy.worker.Worker: org.apache.spark.rpc.RpcEnv startRpcEnvAndEndpoint(java.lang.String,int,int,int,int,java.lang.String[],java.lang.String,scala.Option,org.apache.spark.SparkConf)>
<org.apache.spark.deploy.LocalSparkCluster$$anonfun$start$2: scala.collection.mutable.ArrayBuffer apply(int)>
<org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$org$apache$spark$deploy$client$StandaloneAppClient$ClientEndpoint$$askAndReplyAsync$1: java.lang.Object applyOrElse(java.lang.Object,scala.Function1)>
<org.apache.spark.SparkContext: org.apache.spark.partial.PartialResult runApproximateJob(org.apache.spark.rdd.RDD,scala.Function2,org.apache.spark.partial.ApproximateEvaluator,long)>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$map$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$reduce$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1: scala.collection.mutable.Map apply()>
<org.apache.spark.rdd.RDD$$anonfun$filter$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$keyBy$1: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.SparkContext$$anonfun$sequenceFile$3: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$treeReduce$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreach$1: scala.runtime.BoxedUnit[] apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$flatMapValues$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$collect$2: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1: scala.runtime.BoxedUnit[] apply()>
<org.apache.spark.rdd.RDD$$anonfun$groupBy$3: org.apache.spark.rdd.RDD apply()>
<org.apache.spark.rdd.RDD$$anonfun$flatMap$1: org.apache.spark.rdd.MapPartitionsRDD apply()>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$3: scala.collection.Seq apply(scala.collection.Set)>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$$anonfun$13: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.DAGScheduler: scala.collection.Seq org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal(org.apache.spark.rdd.RDD,int,scala.collection.mutable.HashSet)>
<org.apache.spark.storage.BlockManager: scala.collection.immutable.Map blockIdsToHosts(org.apache.spark.storage.BlockId[],org.apache.spark.SparkEnv,org.apache.spark.storage.BlockManagerMaster)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$6: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$4: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$10: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$7: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$8: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManagerSource$$anonfun$9: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.master.ui.MasterWebUI$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.rest.StatusRequestServlet$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.rest.KillRequestServlet$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.ui.WebUI$$anonfun$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.deploy.LocalSparkCluster$$anonfun$start$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulable accumulable(java.lang.Object,java.lang.String,org.apache.spark.AccumulableParam)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator intAccumulator(int)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator doubleAccumulator(double)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(java.lang.Object,org.apache.spark.AccumulatorParam)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator intAccumulator(int,java.lang.String)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator doubleAccumulator(double,java.lang.String)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(java.lang.Object,java.lang.String,org.apache.spark.AccumulatorParam)>
<org.apache.spark.SparkContext: java.lang.Object runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.reflect.ClassTag)>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$lookup$1: scala.collection.Seq apply()>
<org.apache.spark.rdd.RDD$$anonfun$toLocalIterator$1: java.lang.Object org$apache$spark$rdd$RDD$$anonfun$$collectPartition$1(int)>
<org.apache.spark.rdd.RDD$$anonfun$take$1: java.lang.Object apply()>
<org.apache.spark.api.java.JavaRDDLike$class: java.util.List[] collectPartitions(org.apache.spark.api.java.JavaRDDLike,int[])>
<org.apache.spark.api.python.PythonRDD$: int runJob(org.apache.spark.SparkContext,org.apache.spark.api.java.JavaRDD,java.util.ArrayList)>
<org.apache.spark.rdd.RDD$$anonfun$countByValueApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sumApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.rdd.RDD$$anonfun$countApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$meanApprox$1: org.apache.spark.partial.PartialResult apply()>
<org.apache.spark.ComplexFutureAction$$anon$1: org.apache.spark.FutureAction submitJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.Function2,scala.Function0)>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachPartitionAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$collectAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$countAsync$1: org.apache.spark.SimpleFutureAction apply()>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$map$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$zipPartitions$5: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$filter$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$keyBy$1: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$sequenceFile$3: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreach$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$pipe$3: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$zipPartitions$3: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$flatMapValues$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$collect$2: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$zipPartitions$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKeyWithClassTag$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$groupBy$3: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$flatMap$1: java.lang.Object apply()>
<org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4: java.lang.Object apply()>
<org.apache.spark.SparkContext: boolean killExecutor(java.lang.String)>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend: boolean killExecutor(java.lang.String)>
<org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$3: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.LocalRDDCheckpointData$$anonfun$1: boolean apply(int)>
<org.apache.spark.scheduler.DAGScheduler: scala.collection.Seq getPreferredLocs(org.apache.spark.rdd.RDD,int)>
<org.apache.spark.api.java.JavaPairRDD: org.apache.spark.api.java.JavaPairRDD unpersist()>
<org.apache.spark.api.java.JavaPairRDD: org.apache.spark.api.java.JavaPairRDD unpersist(boolean)>
<org.apache.spark.api.java.JavaDoubleRDD: org.apache.spark.api.java.JavaDoubleRDD unpersist()>
<org.apache.spark.api.java.JavaDoubleRDD: org.apache.spark.api.java.JavaDoubleRDD unpersist(boolean)>
<org.apache.spark.api.java.JavaRDD: org.apache.spark.api.java.JavaRDD unpersist()>
<org.apache.spark.api.java.JavaRDD: org.apache.spark.api.java.JavaRDD unpersist(boolean)>
<org.apache.spark.SparkContext: org.apache.spark.storage.RDDInfo[] getRDDStorageInfo()>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulable accumulable(java.lang.Object,org.apache.spark.AccumulableParam)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(int)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(double)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(int,java.lang.String)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.Accumulator accumulator(double,java.lang.String)>
<org.apache.spark.SparkContext: java.lang.Object runJob(org.apache.spark.rdd.RDD,scala.Function2,scala.reflect.ClassTag)>
<org.apache.spark.rdd.RDD: long count()>
<org.apache.spark.rdd.RDD$$anonfun$collectPartitions$1: java.lang.Object[] apply()>
<org.apache.spark.rdd.RDD$$anonfun$collect$1: java.lang.Object apply()>
<org.apache.spark.rdd.PairRDDFunctions$$anonfun$lookup$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$toLocalIterator$1$$anonfun$apply$30: scala.collection.mutable.ArrayOps apply(int)>
<org.apache.spark.api.java.AbstractJavaRDDLike: java.util.List[] collectPartitions(int[])>
<org.apache.spark.api.python.PythonRDD: int runJob(org.apache.spark.SparkContext,org.apache.spark.api.java.JavaRDD,java.util.ArrayList)>
<org.apache.spark.rdd.RDD$$anonfun$countByValueApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sumApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$countApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$meanApprox$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachPartitionAsync$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$collectAsync$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$countAsync$1: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$hadoopFile$1: java.lang.Object apply()>
<org.apache.spark.rdd.LocalRDDCheckpointData$$anonfun$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: scala.collection.Seq getPreferredLocs(org.apache.spark.rdd.RDD,int)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobSubmitted$5: java.lang.String apply()>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleMapStageSubmitted$5: java.lang.String apply()>
<org.apache.spark.rdd.BlockRDD: scala.collection.Seq getPreferredLocations(org.apache.spark.Partition)>
<org.apache.spark.rdd.BlockRDD: scala.collection.immutable.Map getBlockIdLocations()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$7: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$1: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$5: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$6: long apply$mcJ$sp()>
<org.apache.spark.rdd.RDD$$anonfun$takeSample$1: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$3: long apply$mcJ$sp()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$4: long apply$mcJ$sp()>
<org.apache.spark.api.java.JavaRDDLike$class: long count(org.apache.spark.api.java.JavaRDDLike)>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$2: long apply$mcJ$sp()>
<org.apache.spark.rdd.RDD$$anonfun$collectPartitions$1: java.lang.Object apply()>
<org.apache.spark.rdd.RDD$$anonfun$toLocalIterator$1$$anonfun$apply$30: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.RDD$$anonfun$zipWithIndex$1: java.lang.Object apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$org$apache$spark$rdd$AsyncRDDActions$$anonfun$$continue$1$1: scala.concurrent.Future apply(scala.Unit$)>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$11: scala.concurrent.Future apply(org.apache.spark.JobSubmitter)>
<org.apache.spark.SparkContext$$anonfun$hadoopRDD$1: java.lang.Object apply()>
<org.apache.spark.rdd.DefaultPartitionCoalescer: scala.collection.Seq currPrefLocs(org.apache.spark.Partition,org.apache.spark.rdd.RDD)>
<org.apache.spark.rdd.PartitionerAwareUnionRDD: scala.collection.Seq org$apache$spark$rdd$PartitionerAwareUnionRDD$$currPrefLocs(org.apache.spark.rdd.RDD,org.apache.spark.Partition)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$15: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$16: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobSubmitted$5: java.lang.Object apply()>
<org.apache.spark.scheduler.DAGScheduler$$anonfun$handleMapStageSubmitted$5: java.lang.Object apply()>
<org.apache.spark.rdd.RDD: java.lang.String toDebugString()>
<org.apache.spark.rdd.RDD$$anonfun$38: scala.collection.Seq apply(org.apache.spark.Dependency)>
<org.apache.spark.rdd.ReliableRDDCheckpointData: org.apache.spark.rdd.CheckpointRDD doCheckpoint()>
<org.apache.spark.rdd.ReliableCheckpointRDD: org.apache.spark.rdd.ReliableCheckpointRDD writeRDDToCheckpointDirectory(org.apache.spark.rdd.RDD,java.lang.String,int,scala.reflect.ClassTag)>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$7: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$1: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$5: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$6: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$3: long apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$4: long apply()>
<org.apache.spark.api.java.AbstractJavaRDDLike: long count()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$2: long apply()>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$org$apache$spark$rdd$AsyncRDDActions$$anonfun$$continue$1$1: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$11: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.storage.BlockManager: boolean putBytes(org.apache.spark.storage.BlockId,org.apache.spark.util.io.ChunkedByteBuffer,org.apache.spark.storage.StorageLevel,boolean,scala.reflect.ClassTag)>
<org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1: boolean apply()>
<org.apache.spark.rdd.CoalescedRDDPartition$$anonfun$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations$$anonfun$getAllPrefLocs$2: java.lang.Object apply(org.apache.spark.Partition)>
<org.apache.spark.rdd.DefaultPartitionCoalescer: org.apache.spark.rdd.PartitionGroup pickBin(org.apache.spark.Partition,org.apache.spark.rdd.RDD,double,org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations)>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$5: scala.collection.Seq apply(scala.Tuple2)>
<org.apache.spark.rdd.RDD$$anonfun$preferredLocations$2: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$runJob$2: java.lang.String apply()>
<org.apache.spark.api.java.JavaRDDLike$class: java.lang.String toDebugString(org.apache.spark.api.java.JavaRDDLike)>
<org.apache.spark.rdd.RDD$$anonfun$38: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$7: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$1: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$5: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$6: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$3: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$4: java.lang.Object apply()>
<org.apache.spark.ui.UIWorkloadGenerator$$anonfun$2: java.lang.Object apply()>
<org.apache.spark.storage.BlockManager: boolean putSingle(org.apache.spark.storage.BlockId,java.lang.Object,org.apache.spark.storage.StorageLevel,boolean,scala.reflect.ClassTag)>
<org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1: java.lang.Object apply()>
<org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations$$anonfun$getAllPrefLocs$2: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.rdd.DefaultPartitionCoalescer$$anonfun$throwBalls$9: scala.collection.mutable.ArrayBuffer apply(org.apache.spark.Partition)>
<org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$5: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext$$anonfun$runJob$2: java.lang.Object apply()>
<org.apache.spark.api.java.AbstractJavaRDDLike: java.lang.String toDebugString()>
<org.apache.spark.rdd.DefaultPartitionCoalescer$$anonfun$throwBalls$9: java.lang.Object apply(java.lang.Object)>
<org.apache.spark.SparkContext: org.apache.spark.broadcast.Broadcast broadcast(java.lang.Object,scala.reflect.ClassTag)>
<org.apache.spark.api.java.JavaSparkContext: org.apache.spark.broadcast.Broadcast broadcast(java.lang.Object)>
<org.apache.spark.ShuffleStatus: byte[] serializedMapStatus(org.apache.spark.broadcast.BroadcastManager,boolean,int)>
<org.apache.spark.MapOutputTracker: scala.Tuple2 serializeMapStatuses(org.apache.spark.scheduler.MapStatus[],org.apache.spark.broadcast.BroadcastManager,boolean,int)>
<org.apache.spark.api.python.PythonRDD$: org.apache.spark.broadcast.Broadcast readBroadcastFromFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD sequenceFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD newAPIHadoopFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD newAPIHadoopRDD(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD hadoopFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.api.java.JavaRDD hadoopRDD(org.apache.spark.api.java.JavaSparkContext,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.HashMap,int)>
<org.apache.spark.SparkContext$$anonfun$newAPIHadoopRDD$1: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2: java.lang.Object apply()>
<org.apache.spark.SparkContext$$anonfun$checkpointFile$1: java.lang.Object apply()>
<org.apache.spark.api.python.PythonRDD: org.apache.spark.broadcast.Broadcast readBroadcastFromFile(org.apache.spark.api.java.JavaSparkContext,java.lang.String)>
<org.spark_project.jetty.util.ssl.SslContextFactory: javax.net.ssl.SSLSocket newSslSocket()>
<org.spark_project.jetty.security.authentication.DigestAuthenticator$Digest: boolean check(java.lang.Object)>
<org.spark_project.jetty.security.authentication.FormAuthenticator: org.spark_project.jetty.server.UserIdentity login(java.lang.String,java.lang.Object,javax.servlet.ServletRequest)>
<org.spark_project.jetty.server.Request: java.lang.String getRemoteUser()>
<org.apache.spark.status.api.v1.BaseAppResource$$anonfun$withUI$1: java.lang.Object apply(java.lang.Object)>
<org.apache.commons.crypto.random.OpenSslCryptoRandom: void <init>(java.util.Properties)>
<org.apache.commons.crypto.random.OsCryptoRandom: void <init>(java.util.Properties)>
<org.apache.commons.crypto.random.JavaCryptoRandom: void <init>(java.util.Properties)>
<org.apache.commons.crypto.jna.OpenSslJnaCryptoRandom: void <init>(java.util.Properties)>
<org.apache.wicket.util.crypt.SunJceCrypt: void <init>()>
<org.apache.wicket.util.diff.Diff: java.lang.Object[] randomEdit(java.lang.Object[],long)>
<org.apache.wicket.util.diff.Diff: java.lang.Object[] randomEdit(java.lang.Object[])>
<org.apache.wicket.util.diff.Diff: java.lang.Object[] randomSequence(int,long)>
<org.apache.wicket.util.diff.Diff: java.lang.Object[] randomSequence(int)>
<org.apache.directory.server.kerberos.shared.crypto.encryption.DesCbcCrcEncryption: byte[] encrypt(byte[],byte[])>
<org.apache.directory.server.kerberos.shared.crypto.encryption.DesCbcCrcEncryption: byte[] decrypt(byte[],byte[])>
<org.apache.directory.server.kerberos.shared.crypto.encryption.DesCbcCrcEncryption: byte[] getDecryptedData(org.apache.directory.shared.kerberos.components.EncryptionKey,org.apache.directory.shared.kerberos.components.EncryptedData,org.apache.directory.server.kerberos.shared.crypto.encryption.KeyUsage)>
<org.apache.directory.server.kerberos.shared.crypto.encryption.CipherTextHandler: org.apache.directory.shared.kerberos.components.EncryptedData seal(org.apache.directory.shared.kerberos.components.EncryptionKey,org.apache.directory.api.asn1.Asn1Object,org.apache.directory.server.kerberos.shared.crypto.encryption.KeyUsage)>

<org.apache.directory.server.kerberos.shared.crypto.encryption.DesStringToKey: byte[] getKey(java.lang.String)>
<org.apache.directory.server.kerberos.shared.crypto.encryption.DesStringToKey: byte[] getKey(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.common.io.DigestPrintStream: void <init>(java.io.OutputStream,java.lang.String)>
<org.apache.hadoop.hive.common.io.DigestPrintStream: void <init>(java.io.OutputStream,java.lang.String)>
<org.apache.james.mime4j.storage.CipherStorageProvider: void <init>(org.apache.james.mime4j.storage.StorageProvider)>
<org.apache.kylin.rest.security.PasswordPlaceholderConfigurer: java.lang.String decrypt(java.lang.String)>
<org.apache.kylin.cube.model.CubeDesc: java.lang.String calculateSignature()>
<org.apache.kylin.rest.service.CubeService: org.apache.kylin.cube.CubeInstance createCubeAndDesc(org.apache.kylin.metadata.project.ProjectInstance,org.apache.kylin.cube.model.CubeDesc)>
<org.apache.kylin.rest.service.CubeService: org.apache.kylin.cube.model.CubeDesc updateCubeAndDesc(org.apache.kylin.cube.CubeInstance,org.apache.kylin.cube.model.CubeDesc,java.lang.String,boolean)>
<org.apache.kylin.rest.controller.CubeController: org.apache.kylin.cube.CubeInstance cloneCube(java.lang.String,org.apache.kylin.rest.request.CubeRequest)>
<org.apache.kylin.rest.controller.CubeController: org.apache.kylin.cube.CubeInstance enableCube(java.lang.String)>
<org.apache.kylin.rest.service.JobService: org.apache.kylin.job.JobInstance submitJob(org.apache.kylin.cube.CubeInstance,org.apache.kylin.metadata.model.SegmentRange$TSRange,org.apache.kylin.metadata.model.SegmentRange,java.util.Map,java.util.Map,org.apache.kylin.cube.model.CubeBuildTypeEnum,boolean,java.lang.String)>
<org.apache.kylin.rest.controller.CubeController: org.apache.kylin.job.JobInstance rebuild2(java.lang.String,org.apache.kylin.rest.request.JobBuildRequest2)>
<org.apache.kylin.rest.controller.CubeController: org.apache.kylin.job.JobInstance build(java.lang.String,org.apache.kylin.rest.request.JobBuildRequest)>
<org.apache.maven.wagon.observers.ChecksumObserver: void <init>()>
<org.apache.maven.wagon.observers.ChecksumObserver: void <init>()>
<org.apache.xml.security.stax.impl.processor.input.AbstractDecryptInputProcessor: org.apache.xml.security.stax.ext.stax.XMLSecEvent processNextHeaderEvent(org.apache.xml.security.stax.ext.InputProcessorChain)>
<org.apache.xml.security.stax.impl.processor.input.AbstractDecryptInputProcessor: org.apache.xml.security.stax.ext.stax.XMLSecEvent processNextEvent(org.apache.xml.security.stax.ext.InputProcessorChain)>


<org.apache.xml.security.encryption.XMLCipherUtil: java.security.spec.AlgorithmParameterSpec constructBlockCipherParameters(boolean,byte[],java.lang.Class)>
<org.apache.xml.security.stax.impl.processor.input.XMLEncryptedKeyInputHandler$1$1: org.apache.xml.security.stax.securityToken.InboundSecurityToken getKeyWrappingToken()>
<org.apache.xml.security.stax.impl.processor.input.AbstractDecryptInputProcessor: org.apache.xml.security.stax.ext.stax.XMLSecEvent processNextHeaderEvent(org.apache.xml.security.stax.ext.InputProcessorChain)>
<org.apache.xml.security.stax.impl.processor.input.AbstractDecryptInputProcessor: org.apache.xml.security.stax.ext.stax.XMLSecEvent processNextEvent(org.apache.xml.security.stax.ext.InputProcessorChain)>
<org.apache.xml.security.stax.impl.processor.input.XMLEncryptedKeyInputHandler$1$1: org.apache.xml.security.stax.securityToken.SecurityToken getKeyWrappingToken()>
<org.apache.xml.security.encryption.XMLCipher: org.apache.xml.security.encryption.EncryptedData encryptData(org.w3c.dom.Document,org.w3c.dom.Element)>
<org.apache.xml.security.encryption.XMLCipher: org.w3c.dom.Document doFinal(org.w3c.dom.Document,org.w3c.dom.Document)>
<org.apache.xml.security.encryption.XMLCipher: org.w3c.dom.Document doFinal(org.w3c.dom.Document,org.w3c.dom.Element)>
<org.apache.hadoop.crypto.random.OpensslSecureRandom: void <init>()>
<org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext: org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.fs.LocalDirAllocator: org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.fs.LocalDirAllocator: org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.fs.LocalDirAllocator: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.fs.LocalDirAllocator: org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.io.SequenceFile$Sorter: org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator merge(java.util.List,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.io.SequenceFile$Sorter: org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator merge(org.apache.hadoop.fs.Path[],boolean,int,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.io.SequenceFile$Sorter: org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)>
<org.apache.hadoop.io.SequenceFile$Sorter: org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator sortAndIterate(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)>
<org.apache.hadoop.crypto.random.OsSecureRandom: void <init>()>
<org.apache.hadoop.ipc.Client: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean)>
<org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker: com.google.protobuf.Message invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>
<org.apache.hadoop.ipc.WritableRpcEngine$Invoker: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>
<org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>
<org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext: void <init>(java.lang.String)>
<org.apache.hadoop.fs.LocalDirAllocator: org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.fs.LocalDirAllocator: org.apache.hadoop.fs.Path getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.fs.LocalDirAllocator: java.lang.Iterable getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.fs.LocalDirAllocator: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.fs.LocalDirAllocator: boolean ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.fs.LocalDirAllocator: org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.fs.LocalDirAllocator: org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.io.SequenceFile$Sorter: org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator merge(java.util.List,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.io.SequenceFile$Sorter: org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator merge(org.apache.hadoop.fs.Path[],boolean,int,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.io.SequenceFile$Sorter: org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)>
<org.apache.hadoop.io.SequenceFile$Sorter: org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator sortAndIterate(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)>
<org.apache.hadoop.io.SequenceFile$Writer: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)>
<org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator$1: java.lang.Object call()>
<org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator$2: java.lang.Object call()>
<org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator$5: java.lang.Object call()>
<org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator$4: java.lang.Object call()>
<org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator$6: java.lang.Object call()>
<org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator$3: java.lang.Object call()>
<org.apache.hadoop.ipc.ExternalCall: java.lang.Object run()>